{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Dermatology Image Dataset\n",
    "Skin Disease and Skin Cancer Taxonomy for Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib2\n",
    "import urlparse \n",
    "import re\n",
    "import os.path\n",
    "import cPickle as pickle\n",
    "\n",
    "#Constants - path definitions\n",
    "HOME_PAGE = \"http://www.dermnet.com/dermatology-pictures-skin-disease-pictures/\"\n",
    "DOMAIN_PAGE = \"http://www.dermnet.com/\"\n",
    "IMAGE_DIR = \"/Users/aum/Documents/Analytics/derm_project2/data\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### The Dermnet webpage is structured as follows:\n",
    "\n",
    "* Home Page (Root)\n",
    "    * Class Label Links\n",
    "        * Class Categories Links\n",
    "            * Paginated pages of images\n",
    "\n",
    "Our Approach: traverse through the webpage hierarchy and build a flattened dictionary of (class: list of image links)\n",
    "\n",
    "---\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def openPg(url):\n",
    "    \"\"\"Opens a web page for parsing.\n",
    "    \n",
    "    Args:\n",
    "        url: a web address.\n",
    "    \n",
    "    Returns:\n",
    "        BeautifulSoup object to parse.\n",
    "    \"\"\"\n",
    "    html = urllib2.urlopen(url)\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A function that returns a list of all images for a given skin disease class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getClassImages(class_url):\n",
    "    \"\"\"Returns all images for a class label, including all child (webpage) categories.\n",
    "    \n",
    "    Args:\n",
    "        class_url: a web address for a skin disease class.\n",
    "    \n",
    "    Returns:\n",
    "        class_images: A list containing all image links for a class label.\n",
    "    \"\"\"\n",
    "    class_images = []\n",
    "    cat_urls = getClassCategories(class_url)\n",
    "    for url in cat_urls:\n",
    "        class_images.extend(getCategoryImages(url))\n",
    "    return class_images\n",
    "\n",
    "def getClassCategories(class_url):\n",
    "    \"\"\"Returns all category urls for a skin disease class.\n",
    "    \n",
    "    Args:\n",
    "        class_url: a web address for a skin disease class.\n",
    "    \n",
    "    Returns:\n",
    "        categories: A list containing category urls.\n",
    "    \"\"\"\n",
    "    soup = openPg(class_url)\n",
    "    cat_links = soup.find(\"table\").find_all(\"a\")\n",
    "    categories = []\n",
    "    for link in cat_links:\n",
    "        abs_link = urlparse.urljoin(DOMAIN_PAGE, link.get('href'))\n",
    "        categories.append(abs_link)\n",
    "    return categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Helper functions to get all images within a category. \n",
    "For a given category, we parse through paginated pages of thumbnail images and add the full image path to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getCategoryImages(cat_url):\n",
    "    \"\"\"Captures all category image urls within a series of paginated links.\n",
    "    \n",
    "    Args:\n",
    "        cat_url: a category web address.\n",
    "    \n",
    "    Returns:\n",
    "        cat_images: A list containing image urls.\n",
    "    \"\"\"\n",
    "    cat_images = []\n",
    "    cat_thumbpgs = []\n",
    "    #add to category image list\n",
    "    addPgImages(cat_url, cat_images)\n",
    "    cat_thumbpgs = getAllThumbPgs(cat_url)    \n",
    "    # more pages in category, add images from those thumbnail pages\n",
    "    if cat_thumbpgs: \n",
    "        for page in cat_thumbpgs:\n",
    "            addPgImages(page, cat_images)\n",
    "    return cat_images\n",
    "\n",
    "def getAllThumbPgs(cat_url):\n",
    "    \"\"\"Returns pagnated links associated to a category, if any.\n",
    "    \n",
    "    Args:\n",
    "        cat_url: a category web address.\n",
    "    \n",
    "    Returns:\n",
    "        thumb_pgs: A list of pagnated link addresses.\n",
    "    \"\"\"\n",
    "    soup = openPg(cat_url)\n",
    "    pages = soup.find(\"div\",\"pagination\")\n",
    "    thumb_pgs = []\n",
    "    if pages:  #there are multiple pages for this category\n",
    "        for page in pages:\n",
    "            if page.name == 'a' and page.string <> 'Next':\n",
    "                thumb_pgs.append(urlparse.urljoin(DOMAIN_PAGE, page['href']))\n",
    "    return thumb_pgs\n",
    "\n",
    "def addPgImages(url,image_list):\n",
    "    \"\"\"Finds all image links in a webpage and adds them to the image list.\n",
    "    \n",
    "    Args:\n",
    "        url: a web address for a pagnated category page.\n",
    "        image_list: a list of image urls\n",
    "    \n",
    "    Returns:\n",
    "        Nothing.\n",
    "    \"\"\"\n",
    "    soup = openPg(url)\n",
    "    thumbnails = soup.find_all(\"div\",\"thumbnails\")\n",
    "    if thumbnails: ## there are thumbnails actually on the page\n",
    "        for thumb in thumbnails:\n",
    "            thumb_link = thumb.img['src']\n",
    "            #use full image link instead of thumbnail link\n",
    "            image_link = re.sub(r'Thumb',\"\",thumb_link)\n",
    "            image_list.append(image_link)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to load or create a dictionary of classes and their associated image links. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def createImageDict(dict_file):\n",
    "    \"\"\"Create image dictionary and serialize to disk (pickle). Unpickle to dictionary object if already exists.\n",
    "    \n",
    "    Args:\n",
    "        dict_file: Absolute path + filename of pickle file object.\n",
    "    \n",
    "    Returns:\n",
    "        image_dict: dictionary containing image urls for 23 skin disease classes.\n",
    "    \"\"\"\n",
    "    # load dictionary object hierarchy if pickled file exists\n",
    "    if os.path.exists(dict_file):\n",
    "        print \"Loading image dictionary %s\" % dict_file\n",
    "        with open(dict_file, 'rb') as f:\n",
    "            try:\n",
    "                img_dict = pickle.load(f)\n",
    "                print \"Loaded image dictionary.\"\n",
    "                return img_dict\n",
    "            except:\n",
    "                print \"Failure to load: %s. Creating dictionary. \" % dict_file\n",
    "    \n",
    "    #create dictionary by parsing Dermnet\n",
    "    #open website root directory and get class links\n",
    "    soup = openPg(HOME_PAGE)\n",
    "    class_links = soup.find(\"table\").find_all(\"a\")\n",
    "\n",
    "    print \"Populating image dictionary...\"\n",
    "    img_dict = {}\n",
    "    for link in class_links:\n",
    "        abs_link = urlparse.urljoin(DOMAIN_PAGE, link.get('href'))\n",
    "        class_name = re.sub(r'[^a-z0-9A-Z\\s]+', '', link.string)\n",
    "        #add to final dictionary {class_name: list of image links}\n",
    "        img_dict[class_name] = getClassImages(abs_link)\n",
    "\n",
    "    print \"Image dictionary populated. Total classes: %s\" % len(img_dict)\n",
    "\n",
    "    #save dictionary to pickle file\n",
    "    with open(dict_file, 'wb') as f:\n",
    "        try:\n",
    "            pickle.dump(img_dict, f)\n",
    "            print \"Saved image dictionary to %s\" % dict_file\n",
    "        except:\n",
    "            print \"Failure to save dictionary %s. \\nPlease investigate. \" % dict_file\n",
    "\n",
    "    return img_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main to save images to local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "## load existing class-to-image url dictionary, or scrape website.\n",
    "img_dictionary = createImageDict(os.path.join(IMAGE_DIR,'imageUrls.p'))\n",
    "\n",
    "## Downloading pictures from dictionary\n",
    "for key, class_imgs in img_dictionary.iteritems():\n",
    "\n",
    "    print \"Processing class: %s\" %key\n",
    "\n",
    "    #create class folders, if it doesn't exist\n",
    "    class_path = os.path.join(IMAGE_DIR,key)\n",
    "    if not os.path.exists(class_path):\n",
    "        print \"Creating dir in: %s\" %class_path\n",
    "        os.mkdir(class_path)\n",
    "\n",
    "    #check if more images to be added to class dir\n",
    "    num_dirImgs = len([name for name in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, name))])\n",
    "    count = 0\n",
    "    if not num_dirImgs == len(class_imgs):\n",
    "        for img in class_imgs:\n",
    "\n",
    "            img_name = os.path.basename(img)\n",
    "            file_name = os.path.join(class_path,img_name)\n",
    "\n",
    "            if os.path.isfile(file_name):\n",
    "                print \"Skipping: \" + img_name + \" has already downloaded.\"\n",
    "            else:\n",
    "                #download image\n",
    "                if class_imgs.index(img) % 10 == 0:\n",
    "                    print \"Downloading image #%s: %s\" %(class_imgs.index(img),img_name)\n",
    "                f = urllib2.urlopen(img).read()\n",
    "                open(file_name, 'wb').write(f)\n",
    "\n",
    "    print \"Download complete for: %s\" %key\n",
    "\n",
    "print \"Scraping Complete.\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
