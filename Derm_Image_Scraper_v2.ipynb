{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Dermatology Image Dataset\n",
    "Skin Disease and Skin Cancer Taxonomy for Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib2\n",
    "import urlparse \n",
    "import re\n",
    "import os.path\n",
    "import cPickle as pickle\n",
    "\n",
    "#Constants - path definitions\n",
    "HOME_PAGE = \"http://www.dermnet.com/dermatology-pictures-skin-disease-pictures/\"\n",
    "DOMAIN_PAGE = \"http://www.dermnet.com/\"\n",
    "IMAGE_DIR = \"/Users/aum/Documents/Analytics/derm_project2/data\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### The Dermnet webpage is structured as follows:\n",
    "\n",
    "* Home Page (Root)\n",
    "    * Class Label Links\n",
    "        * Class Categories Links\n",
    "            * Paginated pages of images\n",
    "\n",
    "Our Approach: traverse through the webpage hierarchy and build a flattened dictionary of (class: list of image links)\n",
    "\n",
    "---\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def openPg(url):\n",
    "    \"\"\"Opens a web page for parsing.\n",
    "    \n",
    "    Args:\n",
    "        url: a web address.\n",
    "    \n",
    "    Returns:\n",
    "        BeautifulSoup object to parse.\n",
    "    \"\"\"\n",
    "    html = urllib2.urlopen(url)\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A function that returns a list of all images for a given skin disease class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getClassImages(class_url):\n",
    "    \"\"\"Returns all images for a class label, including all child (webpage) categories.\n",
    "    \n",
    "    Args:\n",
    "        class_url: a web address for a skin disease class.\n",
    "    \n",
    "    Returns:\n",
    "        class_images: A list containing all image links for a class label.\n",
    "    \"\"\"\n",
    "    class_images = []\n",
    "    cat_urls = getClassCategories(class_url)\n",
    "    for url in cat_urls:\n",
    "        class_images.extend(getCategoryImages(url))\n",
    "    return class_images\n",
    "\n",
    "def getClassCategories(class_url):\n",
    "    \"\"\"Returns all category urls for a skin disease class.\n",
    "    \n",
    "    Args:\n",
    "        class_url: a web address for a skin disease class.\n",
    "    \n",
    "    Returns:\n",
    "        categories: A list containing category urls.\n",
    "    \"\"\"\n",
    "    soup = openPg(class_url)\n",
    "    cat_links = soup.find(\"table\").find_all(\"a\")\n",
    "    categories = []\n",
    "    for link in cat_links:\n",
    "        abs_link = urlparse.urljoin(DOMAIN_PAGE, link.get('href'))\n",
    "        categories.append(abs_link)\n",
    "    return categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Helper functions to get all images within a category. \n",
    "For a given category, we parse through paginated pages of thumbnail images and add the full image path to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getCategoryImages(cat_url):\n",
    "    \"\"\"Captures all category image urls within a series of paginated links.\n",
    "    \n",
    "    Args:\n",
    "        cat_url: a category web address.\n",
    "    \n",
    "    Returns:\n",
    "        cat_images: A list containing image urls.\n",
    "    \"\"\"\n",
    "    cat_images = []\n",
    "    cat_thumbpgs = []\n",
    "    #add to category image list\n",
    "    addPgImages(cat_url, cat_images)\n",
    "    cat_thumbpgs = getAllThumbPgs(cat_url)    \n",
    "    # more pages in category, add images from those thumbnail pages\n",
    "    if cat_thumbpgs: \n",
    "        for page in cat_thumbpgs:\n",
    "            addPgImages(page, cat_images)\n",
    "    return cat_images\n",
    "\n",
    "def getAllThumbPgs(cat_url):\n",
    "    \"\"\"Returns pagnated links associated to a category, if any.\n",
    "    \n",
    "    Args:\n",
    "        cat_url: a category web address.\n",
    "    \n",
    "    Returns:\n",
    "        thumb_pgs: A list of pagnated link addresses.\n",
    "    \"\"\"\n",
    "    soup = openPg(cat_url)\n",
    "    pages = soup.find(\"div\",\"pagination\")\n",
    "    thumb_pgs = []\n",
    "    if pages:  #there are multiple pages for this category\n",
    "        for page in pages:\n",
    "            if page.name == 'a' and page.string <> 'Next':\n",
    "                thumb_pgs.append(urlparse.urljoin(DOMAIN_PAGE, page['href']))\n",
    "    return thumb_pgs\n",
    "\n",
    "def addPgImages(url,image_list):\n",
    "    \"\"\"Finds all image links in a webpage and adds them to the image list.\n",
    "    \n",
    "    Args:\n",
    "        url: a web address for a pagnated category page.\n",
    "        image_list: a list of image urls\n",
    "    \n",
    "    Returns:\n",
    "        Nothing.\n",
    "    \"\"\"\n",
    "    soup = openPg(url)\n",
    "    thumbnails = soup.find_all(\"div\",\"thumbnails\")\n",
    "    if thumbnails: ## there are thumbnails actually on the page\n",
    "        for thumb in thumbnails:\n",
    "            thumb_link = thumb.img['src']\n",
    "            #use full image link instead of thumbnail link\n",
    "            image_link = re.sub(r'Thumb',\"\",thumb_link)\n",
    "            image_list.append(image_link)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to load or create a dictionary of classes and their associated image links. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def createImageDict(dict_file):\n",
    "    \"\"\"Create image dictionary and serialize to disk (pickle). Unpickle to dictionary object if already exists.\n",
    "    \n",
    "    Args:\n",
    "        dict_file: Absolute path + filename of pickle file object.\n",
    "    \n",
    "    Returns:\n",
    "        image_dict: dictionary containing image urls for 23 skin disease classes.\n",
    "    \"\"\"\n",
    "    # load dictionary object hierarchy if pickled file exists\n",
    "    if os.path.exists(dict_file):\n",
    "        print \"Loading image dictionary %s\" % dict_file\n",
    "        with open(dict_file, 'rb') as f:\n",
    "            try:\n",
    "                img_dict = pickle.load(f)\n",
    "                print \"Loaded image dictionary.\"\n",
    "                return img_dict\n",
    "            except:\n",
    "                print \"Failure to load: %s. Creating dictionary. \" % dict_file\n",
    "    \n",
    "    #create dictionary by parsing Dermnet\n",
    "    #open website root directory and get class links\n",
    "    soup = openPg(HOME_PAGE)\n",
    "    class_links = soup.find(\"table\").find_all(\"a\")\n",
    "\n",
    "    print \"Populating image dictionary...\"\n",
    "    img_dict = {}\n",
    "    for link in class_links:\n",
    "        abs_link = urlparse.urljoin(DOMAIN_PAGE, link.get('href'))\n",
    "        class_name = re.sub(r'[^a-z0-9A-Z\\s]+', '', link.string)\n",
    "        #add to final dictionary {class_name: list of image links}\n",
    "        img_dict[class_name] = getClassImages(abs_link)\n",
    "\n",
    "    print \"Image dictionary populated. Total classes: %s\" % len(img_dict)\n",
    "\n",
    "    #save dictionary to pickle file\n",
    "    with open(dict_file, 'wb') as f:\n",
    "        try:\n",
    "            pickle.dump(img_dict, f)\n",
    "            print \"Saved image dictionary to %s\" % dict_file\n",
    "        except:\n",
    "            print \"Failure to save dictionary %s. \\nPlease investigate. \" % dict_file\n",
    "\n",
    "    return img_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main to save images to local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating image dictionary...\n",
      "Addding class Acne and Rosacea Photos\n",
      "Image dictionary populated. Total classes: 1\n",
      "Saved image dictionary to /Users/aum/Documents/Analytics/derm_project2/data/imageUrls.p\n",
      "Processing class: Acne and Rosacea Photos\n",
      "Skipping: acne-closed-comedo-002.jpg has already downloaded.\n",
      "Skipping: acne-closed-comedo-003.jpg has already downloaded.\n",
      "Skipping: acne-closed-comedo-1.jpg has already downloaded.\n",
      "Skipping: acne-closed-comedo-1.jpg has already downloaded.\n",
      "Skipping: acne-closed-comedo-10.jpg has already downloaded.\n",
      "Skipping: acne-closed-comedo-11.jpg has already downloaded.\n",
      "Skipping: acne-closed-comedo-12.jpg has already downloaded.\n",
      "Skipping: acne-closed-comedo-13.jpg has already downloaded.\n",
      "Skipping: acne-closed-comedo-14.jpg has already downloaded.\n",
      "Skipping: acne-closed-comedo-15.jpg has already downloaded.\n",
      "Skipping: acne-closed-comedo-16.jpg has already downloaded.\n",
      "Skipping: acne-closed-comedo-17.jpg has already downloaded.\n",
      "Skipping: acne-closed-comedo-18.jpg has already downloaded.\n",
      "Skipping: acne-closed-comedo-19.jpg has already downloaded.\n",
      "Skipping: acne-closed-comedo-2.jpg has already downloaded.\n",
      "Skipping: acne-closed-comedo-20.jpg has already downloaded.\n",
      "Skipping: acne-closed-comedo-21.jpg has already downloaded.\n",
      "Skipping: acne-closed-comedo-22.jpg has already downloaded.\n",
      "Skipping: acne-closed-comedo-23.jpg has already downloaded.\n",
      "Skipping: acne-closed-comedo-24.jpg has already downloaded.\n",
      "Skipping: acne-closed-comedo-25.jpg has already downloaded.\n",
      "Skipping: acne-closed-comedo-26.jpg has already downloaded.\n",
      "Skipping: acne-closed-comedo-27.jpg has already downloaded.\n",
      "Skipping: acne-closed-comedo-28.jpg has already downloaded.\n",
      "Skipping: acne-closed-comedo-29.jpg has already downloaded.\n",
      "Skipping: acne-closed-comedo-3.jpg has already downloaded.\n",
      "Skipping: acne-closed-comedo-31.jpg has already downloaded.\n",
      "Skipping: acne-closed-comedo-32.jpg has already downloaded.\n",
      "Skipping: acne-closed-comedo-33.jpg has already downloaded.\n",
      "Skipping: acne-closed-comedo-34.jpg has already downloaded.\n",
      "Skipping: acne-closed-comedo-35.jpg has already downloaded.\n",
      "Skipping: acne-closed-comedo-36.jpg has already downloaded.\n",
      "Skipping: acne-closed-comedo-38.jpg has already downloaded.\n",
      "Skipping: acne-closed-comedo-39.jpg has already downloaded.\n",
      "Skipping: acne-closed-comedo-4.jpg has already downloaded.\n",
      "Skipping: acne-closed-comedo-40.jpg has already downloaded.\n",
      "Skipping: acne-closed-comedo-5.jpg has already downloaded.\n",
      "Skipping: acne-closed-comedo-6.jpg has already downloaded.\n",
      "Skipping: acne-closed-comedo-7.jpg has already downloaded.\n",
      "Skipping: acne-closed-comedo-8.jpg has already downloaded.\n",
      "Skipping: acne-closed-comedo-9.jpg has already downloaded.\n",
      "Skipping: acne-Closed-Comedo.jpg has already downloaded.\n",
      "Skipping: Comedones-Ears.jpg has already downloaded.\n",
      "Skipping: ComedonesEars-1.jpg has already downloaded.\n",
      "Skipping: acne-cystic-1.jpg has already downloaded.\n",
      "Skipping: acne-cystic-10.jpg has already downloaded.\n",
      "Skipping: acne-cystic-100.jpg has already downloaded.\n",
      "Skipping: acne-cystic-101.jpg has already downloaded.\n",
      "Skipping: acne-cystic-102.jpg has already downloaded.\n",
      "Skipping: acne-cystic-103.jpg has already downloaded.\n",
      "Skipping: acne-cystic-104.jpg has already downloaded.\n",
      "Skipping: acne-cystic-105.jpg has already downloaded.\n",
      "Skipping: acne-cystic-106.jpg has already downloaded.\n",
      "Skipping: acne-cystic-107.jpg has already downloaded.\n",
      "Skipping: acne-cystic-108.jpg has already downloaded.\n",
      "Skipping: acne-cystic-109.jpg has already downloaded.\n",
      "Skipping: acne-cystic-11.jpg has already downloaded.\n",
      "Skipping: acne-cystic-110.jpg has already downloaded.\n",
      "Skipping: acne-cystic-111.jpg has already downloaded.\n",
      "Skipping: acne-cystic-112.jpg has already downloaded.\n",
      "Skipping: acne-cystic-113.jpg has already downloaded.\n",
      "Skipping: acne-cystic-114.jpg has already downloaded.\n",
      "Skipping: acne-cystic-115.jpg has already downloaded.\n",
      "Skipping: acne-cystic-116.jpg has already downloaded.\n",
      "Skipping: acne-cystic-117.jpg has already downloaded.\n",
      "Skipping: acne-cystic-118.jpg has already downloaded.\n",
      "Skipping: acne-cystic-119.jpg has already downloaded.\n",
      "Skipping: acne-cystic-12.jpg has already downloaded.\n",
      "Skipping: acne-cystic-120.jpg has already downloaded.\n",
      "Skipping: acne-cystic-121.jpg has already downloaded.\n",
      "Skipping: acne-cystic-122.jpg has already downloaded.\n",
      "Skipping: acne-cystic-123.jpg has already downloaded.\n",
      "Skipping: acne-cystic-124.jpg has already downloaded.\n",
      "Skipping: acne-cystic-125.jpg has already downloaded.\n",
      "Skipping: acne-cystic-126.jpg has already downloaded.\n",
      "Skipping: acne-cystic-127.jpg has already downloaded.\n",
      "Skipping: acne-cystic-128.jpg has already downloaded.\n",
      "Skipping: acne-cystic-129.jpg has already downloaded.\n",
      "Skipping: acne-cystic-13.jpg has already downloaded.\n",
      "Skipping: acne-cystic-130.jpg has already downloaded.\n",
      "Skipping: acne-cystic-131.jpg has already downloaded.\n",
      "Skipping: acne-cystic-132.jpg has already downloaded.\n",
      "Skipping: acne-cystic-133.jpg has already downloaded.\n",
      "Skipping: acne-cystic-134.jpg has already downloaded.\n",
      "Skipping: acne-cystic-135.jpg has already downloaded.\n",
      "Skipping: acne-cystic-136.jpg has already downloaded.\n",
      "Skipping: acne-cystic-137.jpg has already downloaded.\n",
      "Skipping: acne-cystic-138.jpg has already downloaded.\n",
      "Skipping: acne-cystic-139.jpg has already downloaded.\n",
      "Skipping: acne-cystic-14.jpg has already downloaded.\n",
      "Skipping: acne-cystic-140.jpg has already downloaded.\n",
      "Skipping: acne-cystic-141.jpg has already downloaded.\n",
      "Skipping: acne-cystic-142.jpg has already downloaded.\n",
      "Skipping: acne-cystic-143.jpg has already downloaded.\n",
      "Skipping: acne-cystic-144.jpg has already downloaded.\n",
      "Skipping: acne-cystic-145.jpg has already downloaded.\n",
      "Skipping: acne-cystic-146.jpg has already downloaded.\n",
      "Skipping: acne-cystic-147.jpg has already downloaded.\n",
      "Skipping: acne-cystic-148.jpg has already downloaded.\n",
      "Skipping: acne-cystic-149.jpg has already downloaded.\n",
      "Skipping: acne-cystic-15.jpg has already downloaded.\n",
      "Skipping: acne-cystic-150.jpg has already downloaded.\n",
      "Downloading image #110: acne-cystic-24.jpg\n",
      "Downloading image #120: acne-cystic-33.jpg\n",
      "Downloading image #130: acne-cystic-44.jpg\n",
      "Downloading image #140: acne-cystic-53.jpg\n",
      "Downloading image #150: acne-cystic-84.jpg\n",
      "Downloading image #160: acne-cystic-93.jpg\n",
      "Downloading image #170: acne-excoriated-12.jpg\n",
      "Downloading image #180: acne-excoriated-21.jpg\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-3e2f177bee37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mclass_imgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Downloading image #%s: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_imgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                 \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/aum/anaconda/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/aum/anaconda/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/aum/anaconda/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    382\u001b[0m                 \u001b[0;31m# fragmentation issues on many platforms.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "## load existing class-to-image url dictionary, or scrape website.\n",
    "img_dictionary = createImageDict(os.path.join(IMAGE_DIR,'imageUrls.p'))\n",
    "\n",
    "## Downloading pictures from dictionary\n",
    "for key, class_imgs in img_dictionary.iteritems():\n",
    "\n",
    "    print \"Processing class: %s\" %key\n",
    "\n",
    "    #create class folders, if it doesn't exist\n",
    "    class_path = os.path.join(IMAGE_DIR,key)\n",
    "    if not os.path.exists(class_path):\n",
    "        print \"Creating dir in: %s\" %class_path\n",
    "        os.mkdir(class_path)\n",
    "\n",
    "    #check if more images to be added to class dir\n",
    "    num_dirImgs = len([name for name in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, name))])\n",
    "    count = 0\n",
    "    if not num_dirImgs == len(class_imgs):\n",
    "        for img in class_imgs:\n",
    "\n",
    "            img_name = os.path.basename(img)\n",
    "            file_name = os.path.join(class_path,img_name)\n",
    "\n",
    "            if os.path.isfile(file_name):\n",
    "                print \"Skipping: \" + img_name + \" has already downloaded.\"\n",
    "            else:\n",
    "                #download image\n",
    "                if class_imgs.index(img) % 10 == 0:\n",
    "                    print \"Downloading image #%s: %s\" %(class_imgs.index(img),img_name)\n",
    "                f = urllib2.urlopen(img).read()\n",
    "                open(file_name, 'wb').write(f)\n",
    "\n",
    "    print \"Download complete for: %s\" %key\n",
    "\n",
    "print \"Scraping Complete.\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
